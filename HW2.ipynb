{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiZaman/Spring2019-CAP5610/blob/master/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Y2HofE0pHJZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**************HW 2**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The goal of this homework is to create a convolutional neural network for the CIFAR10 data set. See this colab notebook how to load the CIFAR data in Keras.\n",
        "\n",
        "You should not use any pretrained convnets that come with Keras. You have to create and train your own convnets with Keras from scratch.\n",
        "\n",
        "\n",
        "\n",
        "*   Simple hold-out validation\n",
        "\n",
        "Make sure that the data is divided into:\n",
        "\n",
        "training set (80%)\n",
        "validation set (20%)\n",
        "test set.\n",
        "Use the training set to train your neural networks. Evaluate their performance on the validation data set.\n",
        "\n",
        "After trying several different architectures, choose the one that performs best of the validation set. Try at least four different architectures by using data augmentation, using dropout, varying the number of layers, the number of filters, etc.\n",
        "\n",
        "Train this final architecture on the data from the training set and validation set and evaluate its performance on the test set.\n",
        "\n",
        "*   k-fold validation\n",
        "\n",
        "Reevaluate your best architecture using k-fold validation with k=5, that is, the size of the validation fold is 20%. Does the accuracy/loss obtain by k-fold validation differ from the accuracy/loss obtain by simple hold-out validation."
      ]
    },
    {
      "metadata": {
        "id": "6Alftzj_PcHY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Libraries and Loading Dataset**"
      ]
    },
    {
      "metadata": {
        "id": "ZCbODkRdEWWV",
        "colab_type": "code",
        "outputId": "62b64aab-dd8c-402b-a7cd-ecb972ad5c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import regularizers\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels) = cifar10.load_data()\n",
        "# (train_images_original, train_labels_original), (test_images_original, test_labels_n) = cifar10.load_data() #categorical"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 33s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eGoDyDqmPlV3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reshaping and Normalizing Training and Test Samples**"
      ]
    },
    {
      "metadata": {
        "id": "0uXQebz9lWmi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images_rshp = train_images_original.reshape((train_images_original.shape[0], train_images_original.shape[1], train_images_original.shape[2], train_images_original.shape[3]))\n",
        "test_images_rshp = test_images_original.reshape((test_images_original.shape[0], test_images_original.shape[1], test_images_original.shape[2], test_images_original.shape[3]))\n",
        "\n",
        "train_images_n = train_images_rshp.astype('float32') / 255.0\n",
        "test_images = test_images_rshp.astype('float32') / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7wdFvhgPwYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Simple Hold-out Validation***"
      ]
    },
    {
      "metadata": {
        "id": "F32NQol4aA57",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "indices = np.random.permutation(train_images_original.shape[0])\n",
        "val_indcs = indices[0:10000]\n",
        "val_images = train_images_n[val_indcs]\n",
        "val_labels = train_labels_original[val_indcs]\n",
        "# val_labels_n = train_labels_original[val_indcs] #categorical\n",
        "\n",
        "\n",
        "train_indcs = indices[10000:]\n",
        "train_images = train_images_n[train_indcs]\n",
        "train_labels = train_labels_original[train_indcs]\n",
        "# train_labels_n = train_labels_original[train_indcs] #categorical\n",
        "\n",
        "\n",
        "# train_labels = np_utils.to_categorical(train_labels_n, 10)\n",
        "# val_labels = np_utils.to_categorical(val_labels_n, 10)\n",
        "# test_labels = np_utils.to_categorical(test_labels_n, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L894H3F8_pe8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Initializations\n",
        "\n",
        "weight_decay = 1e-4  #For Kernel Regularizers\n",
        "epochs = 100\n",
        "batch_size=32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fW4lVIVvTHMR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.) **Baseline Model**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DEzuqM-_HKYp",
        "colab_type": "code",
        "outputId": "6dbeea5a-9f3c-41b4-c907-86d64cd090b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4050
        }
      },
      "cell_type": "code",
      "source": [
        "# set up the layers\n",
        "## Baseline\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    #\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    #\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    #\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# compile the model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "history = model.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs,  \n",
        "                      validation_data=(val_images, val_labels))\n",
        "\n",
        "model.save('model1_baseline.h5')\n",
        "print(\"Model saved to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                65600     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 19s 465us/sample - loss: 1.6038 - acc: 0.4144 - val_loss: 1.3693 - val_acc: 0.5066\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 14s 347us/sample - loss: 1.2461 - acc: 0.5555 - val_loss: 1.2152 - val_acc: 0.5661\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 13s 320us/sample - loss: 1.0809 - acc: 0.6211 - val_loss: 1.0914 - val_acc: 0.6127\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 13s 313us/sample - loss: 0.9811 - acc: 0.6559 - val_loss: 0.9999 - val_acc: 0.6520\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 12s 298us/sample - loss: 0.9008 - acc: 0.6842 - val_loss: 0.9489 - val_acc: 0.6688\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 12s 298us/sample - loss: 0.8389 - acc: 0.7060 - val_loss: 0.9276 - val_acc: 0.6780\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 12s 297us/sample - loss: 0.7859 - acc: 0.7268 - val_loss: 0.9125 - val_acc: 0.6853\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 12s 298us/sample - loss: 0.7449 - acc: 0.7389 - val_loss: 0.9075 - val_acc: 0.6929\n",
            "Epoch 9/100\n",
            "40000/40000 [==============================] - 12s 298us/sample - loss: 0.7004 - acc: 0.7538 - val_loss: 0.9481 - val_acc: 0.6849\n",
            "Epoch 10/100\n",
            "40000/40000 [==============================] - 12s 298us/sample - loss: 0.6664 - acc: 0.7639 - val_loss: 0.8943 - val_acc: 0.7027\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 12s 297us/sample - loss: 0.6340 - acc: 0.7767 - val_loss: 0.9226 - val_acc: 0.6939\n",
            "Epoch 12/100\n",
            "40000/40000 [==============================] - 12s 298us/sample - loss: 0.6009 - acc: 0.7899 - val_loss: 0.9052 - val_acc: 0.7052\n",
            "Epoch 13/100\n",
            "40000/40000 [==============================] - 12s 296us/sample - loss: 0.5666 - acc: 0.8006 - val_loss: 0.9434 - val_acc: 0.7006\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 12s 297us/sample - loss: 0.5401 - acc: 0.8073 - val_loss: 0.9728 - val_acc: 0.6944\n",
            "Epoch 15/100\n",
            "40000/40000 [==============================] - 12s 301us/sample - loss: 0.5092 - acc: 0.8196 - val_loss: 0.9658 - val_acc: 0.6982\n",
            "Epoch 16/100\n",
            "40000/40000 [==============================] - 12s 307us/sample - loss: 0.4818 - acc: 0.8259 - val_loss: 1.0187 - val_acc: 0.6908\n",
            "Epoch 17/100\n",
            "40000/40000 [==============================] - 13s 317us/sample - loss: 0.4530 - acc: 0.8382 - val_loss: 1.0557 - val_acc: 0.6857\n",
            "Epoch 18/100\n",
            "40000/40000 [==============================] - 12s 307us/sample - loss: 0.4245 - acc: 0.8489 - val_loss: 1.0654 - val_acc: 0.6925\n",
            "Epoch 19/100\n",
            "40000/40000 [==============================] - 12s 305us/sample - loss: 0.4001 - acc: 0.8561 - val_loss: 1.1232 - val_acc: 0.6906\n",
            "Epoch 20/100\n",
            "40000/40000 [==============================] - 12s 307us/sample - loss: 0.3804 - acc: 0.8631 - val_loss: 1.1338 - val_acc: 0.6894\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 12s 308us/sample - loss: 0.3526 - acc: 0.8718 - val_loss: 1.1922 - val_acc: 0.6957\n",
            "Epoch 22/100\n",
            "40000/40000 [==============================] - 12s 304us/sample - loss: 0.3374 - acc: 0.8782 - val_loss: 1.2721 - val_acc: 0.6781\n",
            "Epoch 23/100\n",
            "40000/40000 [==============================] - 13s 314us/sample - loss: 0.3093 - acc: 0.8888 - val_loss: 1.3390 - val_acc: 0.6798\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 12s 307us/sample - loss: 0.3029 - acc: 0.8885 - val_loss: 1.2915 - val_acc: 0.6810\n",
            "Epoch 25/100\n",
            "40000/40000 [==============================] - 12s 308us/sample - loss: 0.2782 - acc: 0.8985 - val_loss: 1.4887 - val_acc: 0.6761\n",
            "Epoch 26/100\n",
            "40000/40000 [==============================] - 13s 337us/sample - loss: 0.2692 - acc: 0.9022 - val_loss: 1.4284 - val_acc: 0.6767\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 13s 324us/sample - loss: 0.2548 - acc: 0.9070 - val_loss: 1.5708 - val_acc: 0.6591\n",
            "Epoch 28/100\n",
            "40000/40000 [==============================] - 13s 328us/sample - loss: 0.2364 - acc: 0.9146 - val_loss: 1.5186 - val_acc: 0.6775\n",
            "Epoch 29/100\n",
            "40000/40000 [==============================] - 13s 325us/sample - loss: 0.2276 - acc: 0.9181 - val_loss: 1.6405 - val_acc: 0.6708\n",
            "Epoch 30/100\n",
            "40000/40000 [==============================] - 13s 322us/sample - loss: 0.2180 - acc: 0.9208 - val_loss: 1.6372 - val_acc: 0.6724\n",
            "Epoch 31/100\n",
            "40000/40000 [==============================] - 13s 321us/sample - loss: 0.2100 - acc: 0.9232 - val_loss: 1.6222 - val_acc: 0.6761\n",
            "Epoch 32/100\n",
            "40000/40000 [==============================] - 13s 321us/sample - loss: 0.2068 - acc: 0.9268 - val_loss: 1.7428 - val_acc: 0.6653\n",
            "Epoch 33/100\n",
            "40000/40000 [==============================] - 13s 323us/sample - loss: 0.1891 - acc: 0.9305 - val_loss: 1.7389 - val_acc: 0.6734\n",
            "Epoch 34/100\n",
            "40000/40000 [==============================] - 13s 322us/sample - loss: 0.1817 - acc: 0.9350 - val_loss: 1.8449 - val_acc: 0.6717\n",
            "Epoch 35/100\n",
            "40000/40000 [==============================] - 14s 338us/sample - loss: 0.1752 - acc: 0.9370 - val_loss: 1.9258 - val_acc: 0.6691\n",
            "Epoch 36/100\n",
            "40000/40000 [==============================] - 18s 456us/sample - loss: 0.1640 - acc: 0.9395 - val_loss: 1.9140 - val_acc: 0.6753\n",
            "Epoch 37/100\n",
            "40000/40000 [==============================] - 13s 314us/sample - loss: 0.1637 - acc: 0.9401 - val_loss: 1.9711 - val_acc: 0.6685\n",
            "Epoch 38/100\n",
            "40000/40000 [==============================] - 12s 312us/sample - loss: 0.1664 - acc: 0.9419 - val_loss: 2.0046 - val_acc: 0.6710\n",
            "Epoch 39/100\n",
            "40000/40000 [==============================] - 12s 312us/sample - loss: 0.1581 - acc: 0.9431 - val_loss: 2.1331 - val_acc: 0.6629\n",
            "Epoch 40/100\n",
            "40000/40000 [==============================] - 12s 312us/sample - loss: 0.1560 - acc: 0.9441 - val_loss: 2.0723 - val_acc: 0.6664\n",
            "Epoch 41/100\n",
            "40000/40000 [==============================] - 18s 443us/sample - loss: 0.1470 - acc: 0.9472 - val_loss: 2.1242 - val_acc: 0.6723\n",
            "Epoch 42/100\n",
            "40000/40000 [==============================] - 20s 497us/sample - loss: 0.1453 - acc: 0.9475 - val_loss: 2.1601 - val_acc: 0.6661\n",
            "Epoch 43/100\n",
            "40000/40000 [==============================] - 19s 477us/sample - loss: 0.1481 - acc: 0.9481 - val_loss: 2.1487 - val_acc: 0.6714\n",
            "Epoch 44/100\n",
            "40000/40000 [==============================] - 19s 481us/sample - loss: 0.1323 - acc: 0.9532 - val_loss: 2.3302 - val_acc: 0.6511\n",
            "Epoch 45/100\n",
            "40000/40000 [==============================] - 19s 468us/sample - loss: 0.1382 - acc: 0.9508 - val_loss: 2.2231 - val_acc: 0.6655\n",
            "Epoch 46/100\n",
            "40000/40000 [==============================] - 19s 486us/sample - loss: 0.1404 - acc: 0.9501 - val_loss: 2.2974 - val_acc: 0.6659\n",
            "Epoch 47/100\n",
            "40000/40000 [==============================] - 19s 484us/sample - loss: 0.1280 - acc: 0.9544 - val_loss: 2.3038 - val_acc: 0.6733\n",
            "Epoch 48/100\n",
            "40000/40000 [==============================] - 19s 482us/sample - loss: 0.1270 - acc: 0.9564 - val_loss: 2.3520 - val_acc: 0.6682\n",
            "Epoch 49/100\n",
            "40000/40000 [==============================] - 19s 476us/sample - loss: 0.1284 - acc: 0.9553 - val_loss: 2.2715 - val_acc: 0.6688\n",
            "Epoch 50/100\n",
            "40000/40000 [==============================] - 20s 488us/sample - loss: 0.1319 - acc: 0.9545 - val_loss: 2.3996 - val_acc: 0.6656\n",
            "Epoch 51/100\n",
            "40000/40000 [==============================] - 19s 483us/sample - loss: 0.1178 - acc: 0.9596 - val_loss: 2.4680 - val_acc: 0.6672\n",
            "Epoch 52/100\n",
            "40000/40000 [==============================] - 19s 475us/sample - loss: 0.1343 - acc: 0.9545 - val_loss: 2.4895 - val_acc: 0.6611\n",
            "Epoch 53/100\n",
            "40000/40000 [==============================] - 16s 398us/sample - loss: 0.1103 - acc: 0.9619 - val_loss: 2.4763 - val_acc: 0.6658\n",
            "Epoch 54/100\n",
            "40000/40000 [==============================] - 14s 352us/sample - loss: 0.1236 - acc: 0.9576 - val_loss: 2.5104 - val_acc: 0.6632\n",
            "Epoch 55/100\n",
            "40000/40000 [==============================] - 13s 335us/sample - loss: 0.1137 - acc: 0.9601 - val_loss: 2.4842 - val_acc: 0.6619\n",
            "Epoch 56/100\n",
            "40000/40000 [==============================] - 12s 305us/sample - loss: 0.1173 - acc: 0.9602 - val_loss: 2.5161 - val_acc: 0.6593\n",
            "Epoch 57/100\n",
            "40000/40000 [==============================] - 12s 307us/sample - loss: 0.1156 - acc: 0.9586 - val_loss: 2.4646 - val_acc: 0.6650\n",
            "Epoch 58/100\n",
            "40000/40000 [==============================] - 12s 306us/sample - loss: 0.1125 - acc: 0.9620 - val_loss: 2.5736 - val_acc: 0.6657\n",
            "Epoch 59/100\n",
            "40000/40000 [==============================] - 13s 316us/sample - loss: 0.1051 - acc: 0.9644 - val_loss: 2.6094 - val_acc: 0.6606\n",
            "Epoch 60/100\n",
            "40000/40000 [==============================] - 12s 309us/sample - loss: 0.1161 - acc: 0.9613 - val_loss: 2.6839 - val_acc: 0.6587\n",
            "Epoch 61/100\n",
            "40000/40000 [==============================] - 12s 307us/sample - loss: 0.1152 - acc: 0.9610 - val_loss: 2.6546 - val_acc: 0.6613\n",
            "Epoch 62/100\n",
            "40000/40000 [==============================] - 12s 306us/sample - loss: 0.0980 - acc: 0.9664 - val_loss: 2.5506 - val_acc: 0.6656\n",
            "Epoch 63/100\n",
            "40000/40000 [==============================] - 12s 308us/sample - loss: 0.1085 - acc: 0.9640 - val_loss: 2.7004 - val_acc: 0.6621\n",
            "Epoch 64/100\n",
            "40000/40000 [==============================] - 13s 314us/sample - loss: 0.1100 - acc: 0.9622 - val_loss: 2.6444 - val_acc: 0.6612\n",
            "Epoch 65/100\n",
            "40000/40000 [==============================] - 12s 301us/sample - loss: 0.1069 - acc: 0.9636 - val_loss: 2.6550 - val_acc: 0.6643\n",
            "Epoch 66/100\n",
            "40000/40000 [==============================] - 12s 309us/sample - loss: 0.1013 - acc: 0.9655 - val_loss: 2.6980 - val_acc: 0.6606\n",
            "Epoch 67/100\n",
            "40000/40000 [==============================] - 12s 310us/sample - loss: 0.1115 - acc: 0.9629 - val_loss: 2.7679 - val_acc: 0.6601\n",
            "Epoch 68/100\n",
            "40000/40000 [==============================] - 13s 313us/sample - loss: 0.0929 - acc: 0.9676 - val_loss: 2.6736 - val_acc: 0.6587\n",
            "Epoch 69/100\n",
            "40000/40000 [==============================] - 12s 308us/sample - loss: 0.0963 - acc: 0.9681 - val_loss: 2.7589 - val_acc: 0.6590\n",
            "Epoch 70/100\n",
            "40000/40000 [==============================] - 12s 311us/sample - loss: 0.1134 - acc: 0.9622 - val_loss: 2.7579 - val_acc: 0.6595\n",
            "Epoch 71/100\n",
            "40000/40000 [==============================] - 12s 305us/sample - loss: 0.0863 - acc: 0.9716 - val_loss: 2.8246 - val_acc: 0.6553\n",
            "Epoch 72/100\n",
            "40000/40000 [==============================] - 12s 303us/sample - loss: 0.1038 - acc: 0.9660 - val_loss: 2.7953 - val_acc: 0.6668\n",
            "Epoch 73/100\n",
            "40000/40000 [==============================] - 12s 308us/sample - loss: 0.1063 - acc: 0.9650 - val_loss: 2.8589 - val_acc: 0.6614\n",
            "Epoch 74/100\n",
            "40000/40000 [==============================] - 12s 310us/sample - loss: 0.0926 - acc: 0.9690 - val_loss: 2.8940 - val_acc: 0.6562\n",
            "Epoch 75/100\n",
            "40000/40000 [==============================] - 12s 309us/sample - loss: 0.0972 - acc: 0.9682 - val_loss: 2.8411 - val_acc: 0.6594\n",
            "Epoch 76/100\n",
            "40000/40000 [==============================] - 12s 310us/sample - loss: 0.0926 - acc: 0.9693 - val_loss: 2.8849 - val_acc: 0.6569\n",
            "Epoch 77/100\n",
            "40000/40000 [==============================] - 12s 311us/sample - loss: 0.0954 - acc: 0.9689 - val_loss: 2.8262 - val_acc: 0.6626\n",
            "Epoch 78/100\n",
            "40000/40000 [==============================] - 12s 310us/sample - loss: 0.0945 - acc: 0.9689 - val_loss: 2.8376 - val_acc: 0.6658\n",
            "Epoch 79/100\n",
            "40000/40000 [==============================] - 13s 319us/sample - loss: 0.0997 - acc: 0.9674 - val_loss: 2.9385 - val_acc: 0.6565\n",
            "Epoch 80/100\n",
            "40000/40000 [==============================] - 13s 323us/sample - loss: 0.0925 - acc: 0.9692 - val_loss: 2.9349 - val_acc: 0.6605\n",
            "Epoch 81/100\n",
            "40000/40000 [==============================] - 13s 323us/sample - loss: 0.0887 - acc: 0.9699 - val_loss: 2.9118 - val_acc: 0.6667\n",
            "Epoch 82/100\n",
            "40000/40000 [==============================] - 13s 318us/sample - loss: 0.0938 - acc: 0.9693 - val_loss: 2.9477 - val_acc: 0.6636\n",
            "Epoch 83/100\n",
            "40000/40000 [==============================] - 13s 323us/sample - loss: 0.0934 - acc: 0.9692 - val_loss: 2.8829 - val_acc: 0.6675\n",
            "Epoch 84/100\n",
            "40000/40000 [==============================] - 13s 329us/sample - loss: 0.0823 - acc: 0.9727 - val_loss: 2.9184 - val_acc: 0.6592\n",
            "Epoch 85/100\n",
            "40000/40000 [==============================] - 13s 321us/sample - loss: 0.0924 - acc: 0.9704 - val_loss: 2.9437 - val_acc: 0.6621\n",
            "Epoch 86/100\n",
            "40000/40000 [==============================] - 13s 326us/sample - loss: 0.0860 - acc: 0.9719 - val_loss: 2.9668 - val_acc: 0.6584\n",
            "Epoch 87/100\n",
            "40000/40000 [==============================] - 16s 391us/sample - loss: 0.0874 - acc: 0.9712 - val_loss: 2.9432 - val_acc: 0.6637\n",
            "Epoch 88/100\n",
            "40000/40000 [==============================] - 13s 335us/sample - loss: 0.0981 - acc: 0.9679 - val_loss: 2.9959 - val_acc: 0.6521\n",
            "Epoch 89/100\n",
            "40000/40000 [==============================] - 12s 312us/sample - loss: 0.0970 - acc: 0.9696 - val_loss: 2.9283 - val_acc: 0.6601\n",
            "Epoch 90/100\n",
            "40000/40000 [==============================] - 12s 309us/sample - loss: 0.0929 - acc: 0.9697 - val_loss: 3.0978 - val_acc: 0.6576\n",
            "Epoch 91/100\n",
            "40000/40000 [==============================] - 12s 309us/sample - loss: 0.0836 - acc: 0.9727 - val_loss: 3.0367 - val_acc: 0.6548\n",
            "Epoch 92/100\n",
            "40000/40000 [==============================] - 15s 387us/sample - loss: 0.0896 - acc: 0.9712 - val_loss: 3.0625 - val_acc: 0.6559\n",
            "Epoch 93/100\n",
            "40000/40000 [==============================] - 16s 397us/sample - loss: 0.0874 - acc: 0.9718 - val_loss: 2.9656 - val_acc: 0.6668\n",
            "Epoch 94/100\n",
            "40000/40000 [==============================] - 16s 392us/sample - loss: 0.0861 - acc: 0.9719 - val_loss: 2.9745 - val_acc: 0.6645\n",
            "Epoch 95/100\n",
            "40000/40000 [==============================] - 16s 394us/sample - loss: 0.0807 - acc: 0.9737 - val_loss: 3.0526 - val_acc: 0.6597\n",
            "Epoch 96/100\n",
            "40000/40000 [==============================] - 16s 396us/sample - loss: 0.0879 - acc: 0.9714 - val_loss: 2.9882 - val_acc: 0.6657\n",
            "Epoch 97/100\n",
            "40000/40000 [==============================] - 16s 402us/sample - loss: 0.0846 - acc: 0.9731 - val_loss: 3.0190 - val_acc: 0.6644\n",
            "Epoch 98/100\n",
            "40000/40000 [==============================] - 16s 402us/sample - loss: 0.0743 - acc: 0.9765 - val_loss: 3.0052 - val_acc: 0.6618\n",
            "Epoch 99/100\n",
            "40000/40000 [==============================] - 16s 403us/sample - loss: 0.0857 - acc: 0.9729 - val_loss: 3.0270 - val_acc: 0.6647\n",
            "Epoch 100/100\n",
            "40000/40000 [==============================] - 16s 399us/sample - loss: 0.0837 - acc: 0.9735 - val_loss: 3.0603 - val_acc: 0.6628\n",
            "Model saved to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yYuuempsTclh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2.) **Deeper Architecture w/ More Layers and Filters, Drop-Out, and L2 Regularizer**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pI63Qug9Xqs3",
        "colab_type": "code",
        "outputId": "8d9378b6-31d2-4c36-9868-bfbfdd63f0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4243
        }
      },
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape= (32,32,3), strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.25),\n",
        "\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.30),\n",
        "    \n",
        "    keras.layers.Conv2D(128, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.Conv2D(128, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.35),\n",
        "    \n",
        "    keras.layers.Conv2D(256, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.Conv2D(256, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.40),\n",
        "  \n",
        "    keras.layers.Conv2D(512, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.Conv2D(512, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.50),\n",
        "    \n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "# compile the model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "history = model.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs,  \n",
        "                      validation_data=(val_images, val_labels))\n",
        "\n",
        "model.save('model2_wDropout&Regularizer.h5')\n",
        "print(\"Model saved to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 2, 2, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 4,980,010\n",
            "Trainable params: 4,980,010\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 62s 2ms/sample - loss: 2.2864 - acc: 0.1947 - val_loss: 1.9221 - val_acc: 0.3208\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 1.8535 - acc: 0.3368 - val_loss: 1.5841 - val_acc: 0.4173\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.5808 - acc: 0.4456 - val_loss: 1.4229 - val_acc: 0.5114\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 1.4280 - acc: 0.5226 - val_loss: 1.3098 - val_acc: 0.5575\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 1.3338 - acc: 0.5643 - val_loss: 1.2444 - val_acc: 0.5932\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 1.2645 - acc: 0.5977 - val_loss: 1.1130 - val_acc: 0.6469\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.2137 - acc: 0.6197 - val_loss: 1.1409 - val_acc: 0.6492\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.1932 - acc: 0.6332 - val_loss: 1.1009 - val_acc: 0.6616\n",
            "Epoch 9/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.1613 - acc: 0.6504 - val_loss: 1.0892 - val_acc: 0.6717\n",
            "Epoch 10/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 1.1437 - acc: 0.6608 - val_loss: 1.0370 - val_acc: 0.6993\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.1282 - acc: 0.6678 - val_loss: 1.0034 - val_acc: 0.7074\n",
            "Epoch 12/100\n",
            "40000/40000 [==============================] - 58s 1ms/sample - loss: 1.1091 - acc: 0.6775 - val_loss: 1.0025 - val_acc: 0.7150\n",
            "Epoch 13/100\n",
            "40000/40000 [==============================] - 58s 1ms/sample - loss: 1.0987 - acc: 0.6863 - val_loss: 1.0109 - val_acc: 0.7085\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 58s 1ms/sample - loss: 1.0834 - acc: 0.6927 - val_loss: 1.0488 - val_acc: 0.7043\n",
            "Epoch 15/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 1.0716 - acc: 0.7005 - val_loss: 0.9643 - val_acc: 0.7337\n",
            "Epoch 16/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 1.0668 - acc: 0.7077 - val_loss: 0.9712 - val_acc: 0.7343\n",
            "Epoch 17/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 1.0638 - acc: 0.7089 - val_loss: 0.9785 - val_acc: 0.7338\n",
            "Epoch 18/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 1.0525 - acc: 0.7151 - val_loss: 0.9926 - val_acc: 0.7310\n",
            "Epoch 19/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.0501 - acc: 0.7146 - val_loss: 0.9509 - val_acc: 0.7399\n",
            "Epoch 20/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.0407 - acc: 0.7220 - val_loss: 0.9692 - val_acc: 0.7368\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.0396 - acc: 0.7188 - val_loss: 0.9271 - val_acc: 0.7504\n",
            "Epoch 22/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.0322 - acc: 0.7247 - val_loss: 0.9834 - val_acc: 0.7432\n",
            "Epoch 23/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 1.0339 - acc: 0.7265 - val_loss: 0.9308 - val_acc: 0.7570\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 1.0252 - acc: 0.7337 - val_loss: 0.9601 - val_acc: 0.7434\n",
            "Epoch 25/100\n",
            "40000/40000 [==============================] - 58s 1ms/sample - loss: 1.0192 - acc: 0.7319 - val_loss: 0.9204 - val_acc: 0.7552\n",
            "Epoch 26/100\n",
            "40000/40000 [==============================] - 58s 1ms/sample - loss: 1.0157 - acc: 0.7340 - val_loss: 0.9184 - val_acc: 0.7672\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 64s 2ms/sample - loss: 1.0118 - acc: 0.7366 - val_loss: 0.9032 - val_acc: 0.7648\n",
            "Epoch 28/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 1.0191 - acc: 0.7332 - val_loss: 0.9217 - val_acc: 0.7575\n",
            "Epoch 29/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 1.0145 - acc: 0.7376 - val_loss: 0.9040 - val_acc: 0.7660\n",
            "Epoch 30/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 1.0083 - acc: 0.7407 - val_loss: 0.9162 - val_acc: 0.7620\n",
            "Epoch 31/100\n",
            "40000/40000 [==============================] - 56s 1ms/sample - loss: 0.9991 - acc: 0.7469 - val_loss: 0.9924 - val_acc: 0.7495\n",
            "Epoch 32/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 1.0009 - acc: 0.7442 - val_loss: 0.9264 - val_acc: 0.7588\n",
            "Epoch 33/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 1.0006 - acc: 0.7459 - val_loss: 0.9092 - val_acc: 0.7675\n",
            "Epoch 34/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 0.9955 - acc: 0.7467 - val_loss: 0.9151 - val_acc: 0.7736\n",
            "Epoch 35/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 0.9947 - acc: 0.7452 - val_loss: 0.9078 - val_acc: 0.7698\n",
            "Epoch 36/100\n",
            "40000/40000 [==============================] - 56s 1ms/sample - loss: 0.9891 - acc: 0.7513 - val_loss: 0.9001 - val_acc: 0.7731\n",
            "Epoch 37/100\n",
            "40000/40000 [==============================] - 59s 1ms/sample - loss: 0.9932 - acc: 0.7494 - val_loss: 0.9052 - val_acc: 0.7693\n",
            "Epoch 38/100\n",
            "40000/40000 [==============================] - 58s 1ms/sample - loss: 0.9901 - acc: 0.7503 - val_loss: 0.9939 - val_acc: 0.7477\n",
            "Epoch 39/100\n",
            "40000/40000 [==============================] - 59s 1ms/sample - loss: 0.9940 - acc: 0.7520 - val_loss: 0.9341 - val_acc: 0.7600\n",
            "Epoch 40/100\n",
            "40000/40000 [==============================] - 56s 1ms/sample - loss: 0.9725 - acc: 0.7569 - val_loss: 0.8706 - val_acc: 0.7873\n",
            "Epoch 41/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 0.9824 - acc: 0.7553 - val_loss: 0.8872 - val_acc: 0.7818\n",
            "Epoch 42/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 0.9803 - acc: 0.7560 - val_loss: 0.8873 - val_acc: 0.7822\n",
            "Epoch 43/100\n",
            "40000/40000 [==============================] - 56s 1ms/sample - loss: 0.9767 - acc: 0.7560 - val_loss: 0.9333 - val_acc: 0.7684\n",
            "Epoch 44/100\n",
            "40000/40000 [==============================] - 56s 1ms/sample - loss: 0.9673 - acc: 0.7587 - val_loss: 0.8907 - val_acc: 0.7796\n",
            "Epoch 45/100\n",
            "40000/40000 [==============================] - 56s 1ms/sample - loss: 0.9737 - acc: 0.7594 - val_loss: 0.9481 - val_acc: 0.7735\n",
            "Epoch 46/100\n",
            "40000/40000 [==============================] - 56s 1ms/sample - loss: 0.9709 - acc: 0.7629 - val_loss: 0.8962 - val_acc: 0.7769\n",
            "Epoch 47/100\n",
            "40000/40000 [==============================] - 56s 1ms/sample - loss: 0.9699 - acc: 0.7598 - val_loss: 0.9361 - val_acc: 0.7614\n",
            "Epoch 48/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9689 - acc: 0.7627 - val_loss: 0.8595 - val_acc: 0.7889\n",
            "Epoch 49/100\n",
            "40000/40000 [==============================] - 59s 1ms/sample - loss: 0.9709 - acc: 0.7614 - val_loss: 0.9019 - val_acc: 0.7789\n",
            "Epoch 50/100\n",
            "40000/40000 [==============================] - 61s 2ms/sample - loss: 0.9646 - acc: 0.7648 - val_loss: 0.8826 - val_acc: 0.7838\n",
            "Epoch 51/100\n",
            "40000/40000 [==============================] - 58s 1ms/sample - loss: 0.9697 - acc: 0.7608 - val_loss: 0.9203 - val_acc: 0.7691\n",
            "Epoch 52/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 0.9552 - acc: 0.7669 - val_loss: 0.8970 - val_acc: 0.7831\n",
            "Epoch 53/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 0.9665 - acc: 0.7633 - val_loss: 0.9093 - val_acc: 0.7764\n",
            "Epoch 54/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 0.9515 - acc: 0.7663 - val_loss: 0.8711 - val_acc: 0.7948\n",
            "Epoch 55/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 0.9624 - acc: 0.7660 - val_loss: 0.9144 - val_acc: 0.7768\n",
            "Epoch 56/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 0.9589 - acc: 0.7679 - val_loss: 0.8563 - val_acc: 0.7997\n",
            "Epoch 57/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 0.9559 - acc: 0.7688 - val_loss: 0.9406 - val_acc: 0.7679\n",
            "Epoch 58/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 0.9544 - acc: 0.7697 - val_loss: 0.9194 - val_acc: 0.7692\n",
            "Epoch 59/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 0.9498 - acc: 0.7697 - val_loss: 0.8776 - val_acc: 0.7917\n",
            "Epoch 60/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 0.9488 - acc: 0.7720 - val_loss: 0.9006 - val_acc: 0.7840\n",
            "Epoch 61/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 0.9389 - acc: 0.7710 - val_loss: 0.8831 - val_acc: 0.7879\n",
            "Epoch 62/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 0.9453 - acc: 0.7728 - val_loss: 0.8875 - val_acc: 0.7873\n",
            "Epoch 63/100\n",
            "40000/40000 [==============================] - 57s 1ms/sample - loss: 0.9525 - acc: 0.7701 - val_loss: 0.9089 - val_acc: 0.7887\n",
            "Epoch 64/100\n",
            "40000/40000 [==============================] - 58s 1ms/sample - loss: 0.9505 - acc: 0.7728 - val_loss: 0.8863 - val_acc: 0.7869\n",
            "Epoch 65/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 0.9514 - acc: 0.7704 - val_loss: 0.8853 - val_acc: 0.7856\n",
            "Epoch 66/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9375 - acc: 0.7763 - val_loss: 0.8412 - val_acc: 0.8003\n",
            "Epoch 67/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9451 - acc: 0.7746 - val_loss: 0.9369 - val_acc: 0.7709\n",
            "Epoch 68/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9476 - acc: 0.7722 - val_loss: 0.9131 - val_acc: 0.7775\n",
            "Epoch 69/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9504 - acc: 0.7736 - val_loss: 0.8650 - val_acc: 0.7889\n",
            "Epoch 70/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9404 - acc: 0.7732 - val_loss: 0.8873 - val_acc: 0.7887\n",
            "Epoch 71/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9515 - acc: 0.7717 - val_loss: 0.8668 - val_acc: 0.7974\n",
            "Epoch 72/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9458 - acc: 0.7733 - val_loss: 0.8797 - val_acc: 0.7867\n",
            "Epoch 73/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9311 - acc: 0.7781 - val_loss: 0.8681 - val_acc: 0.7909\n",
            "Epoch 74/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9410 - acc: 0.7771 - val_loss: 0.8652 - val_acc: 0.7982\n",
            "Epoch 75/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9394 - acc: 0.7772 - val_loss: 0.8894 - val_acc: 0.7899\n",
            "Epoch 76/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9327 - acc: 0.7795 - val_loss: 0.8817 - val_acc: 0.7878\n",
            "Epoch 77/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9344 - acc: 0.7765 - val_loss: 0.8547 - val_acc: 0.7956\n",
            "Epoch 78/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9363 - acc: 0.7782 - val_loss: 0.8750 - val_acc: 0.7921\n",
            "Epoch 79/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9316 - acc: 0.7792 - val_loss: 0.8953 - val_acc: 0.7837\n",
            "Epoch 80/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9320 - acc: 0.7773 - val_loss: 0.8847 - val_acc: 0.7911\n",
            "Epoch 81/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9312 - acc: 0.7802 - val_loss: 0.8770 - val_acc: 0.7977\n",
            "Epoch 82/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9435 - acc: 0.7753 - val_loss: 0.8477 - val_acc: 0.7995\n",
            "Epoch 83/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9359 - acc: 0.7775 - val_loss: 0.9123 - val_acc: 0.7787\n",
            "Epoch 84/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9464 - acc: 0.7755 - val_loss: 0.8527 - val_acc: 0.7990\n",
            "Epoch 85/100\n",
            "40000/40000 [==============================] - 53s 1ms/sample - loss: 0.9348 - acc: 0.7774 - val_loss: 0.8506 - val_acc: 0.7973\n",
            "Epoch 86/100\n",
            "40000/40000 [==============================] - 59s 1ms/sample - loss: 0.9324 - acc: 0.7801 - val_loss: 0.9114 - val_acc: 0.7886\n",
            "Epoch 87/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 0.9356 - acc: 0.7795 - val_loss: 0.8717 - val_acc: 0.7934\n",
            "Epoch 88/100\n",
            "40000/40000 [==============================] - 54s 1ms/sample - loss: 0.9236 - acc: 0.7822 - val_loss: 0.9017 - val_acc: 0.7786\n",
            "Epoch 89/100\n",
            "40000/40000 [==============================] - 55s 1ms/sample - loss: 0.9301 - acc: 0.7795 - val_loss: 0.9052 - val_acc: 0.7921\n",
            "Epoch 90/100\n",
            "29792/40000 [=====================>........] - ETA: 13s - loss: 0.9427 - acc: 0.7772Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TQ0BHI0IbwQj",
        "colab_type": "code",
        "outputId": "6e302095-66c5-4d5e-d8c4-fd12c4a92fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "model.save('model2_wDropout&Regularizer.h5')\n",
        "print(\"Model saved to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F1XhcW6I-IrC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3) Adding Data Augmentation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mG8t5RlFS9PG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.15,\n",
        "        height_shift_range=0.15,\n",
        "        shear_range=0.15,\n",
        "        zoom_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "datagen.fit(train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5xQyvkb2_Fnq",
        "colab_type": "code",
        "outputId": "550d21e3-0bfa-41a3-8568-a70a42e986b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5563
        }
      },
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape= (32,32,3), strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "#     keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.25),\n",
        "\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "#     keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.30),\n",
        "    \n",
        "    keras.layers.Conv2D(128, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "#     keras.layers.Conv2D(128, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.35),\n",
        "    \n",
        "    keras.layers.Conv2D(256, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "#     keras.layers.Conv2D(256, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.40),\n",
        "  \n",
        "    keras.layers.Conv2D(512, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "#     keras.layers.Conv2D(512, (3,3), activation='relu', padding='same', strides= (1,1), kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "    keras.layers.MaxPooling2D((2, 2), padding= 'same'),\n",
        "    keras.layers.Dropout(0.50),\n",
        "    \n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(datagen.flow(train_images, train_labels, batch_size=32), steps_per_epoch=train_images.shape[0] // batch_size, epochs= epochs, verbose=1, validation_data=(val_images,val_labels))\n",
        "\n",
        "model.save('model3_wDataAugmentation.h5')\n",
        "print(\"Model saved to disk\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 2, 2, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,836,362\n",
            "Trainable params: 1,836,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "10000/10000 [==============================] - 3s 272us/sample - loss: 1.9183 - acc: 0.3242\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 2.2471 - acc: 0.2248 - val_loss: 1.9186 - val_acc: 0.3242\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 2s 249us/sample - loss: 1.6707 - acc: 0.4315\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.8822 - acc: 0.3541 - val_loss: 1.6711 - val_acc: 0.4315\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 3s 259us/sample - loss: 1.5813 - acc: 0.4714\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 1.7529 - acc: 0.4111 - val_loss: 1.5817 - val_acc: 0.4714\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 2s 247us/sample - loss: 1.5383 - acc: 0.5070\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.6845 - acc: 0.4489 - val_loss: 1.5384 - val_acc: 0.5070\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 2s 247us/sample - loss: 1.5571 - acc: 0.4919\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.6426 - acc: 0.4720 - val_loss: 1.5573 - val_acc: 0.4919\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 3s 256us/sample - loss: 1.4786 - acc: 0.5380\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.6246 - acc: 0.4886 - val_loss: 1.4788 - val_acc: 0.5380\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 3s 252us/sample - loss: 1.4582 - acc: 0.5485\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.6073 - acc: 0.4979 - val_loss: 1.4584 - val_acc: 0.5485\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 2s 247us/sample - loss: 1.5182 - acc: 0.5294\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5989 - acc: 0.5052 - val_loss: 1.5184 - val_acc: 0.5294\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 3s 253us/sample - loss: 1.4592 - acc: 0.5474\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5884 - acc: 0.5160 - val_loss: 1.4593 - val_acc: 0.5474\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 3s 250us/sample - loss: 1.5324 - acc: 0.5297\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5807 - acc: 0.5230 - val_loss: 1.5326 - val_acc: 0.5297\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 3s 251us/sample - loss: 1.3968 - acc: 0.5867\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5778 - acc: 0.5297 - val_loss: 1.3973 - val_acc: 0.5867\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 2s 246us/sample - loss: 1.3545 - acc: 0.5987\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5707 - acc: 0.5311 - val_loss: 1.3547 - val_acc: 0.5987\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 2s 244us/sample - loss: 1.3065 - acc: 0.6273\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5618 - acc: 0.5368 - val_loss: 1.3066 - val_acc: 0.6273\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 2s 250us/sample - loss: 1.4684 - acc: 0.5705\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5614 - acc: 0.5411 - val_loss: 1.4687 - val_acc: 0.5705\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 3s 250us/sample - loss: 1.4415 - acc: 0.5746\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 1.5624 - acc: 0.5446 - val_loss: 1.4418 - val_acc: 0.5746\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 3s 250us/sample - loss: 1.3702 - acc: 0.5986\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5495 - acc: 0.5523 - val_loss: 1.3704 - val_acc: 0.5986\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 3s 250us/sample - loss: 1.2835 - acc: 0.6409\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5506 - acc: 0.5530 - val_loss: 1.2840 - val_acc: 0.6409\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 2s 248us/sample - loss: 1.4623 - acc: 0.5775\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5489 - acc: 0.5578 - val_loss: 1.4625 - val_acc: 0.5775\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 3s 253us/sample - loss: 1.3037 - acc: 0.6253\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5487 - acc: 0.5591 - val_loss: 1.3037 - val_acc: 0.6253\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 2s 247us/sample - loss: 1.3731 - acc: 0.6211\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5372 - acc: 0.5628 - val_loss: 1.3733 - val_acc: 0.6211\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 2s 233us/sample - loss: 1.3521 - acc: 0.6267\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5387 - acc: 0.5646 - val_loss: 1.3525 - val_acc: 0.6267\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 2s 246us/sample - loss: 1.3036 - acc: 0.6390\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5329 - acc: 0.5666 - val_loss: 1.3038 - val_acc: 0.6390\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 3s 250us/sample - loss: 1.2712 - acc: 0.6488\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5406 - acc: 0.5653 - val_loss: 1.2714 - val_acc: 0.6488\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 2s 249us/sample - loss: 1.4172 - acc: 0.6008\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5205 - acc: 0.5750 - val_loss: 1.4174 - val_acc: 0.6008\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 2s 248us/sample - loss: 1.2872 - acc: 0.6455\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5252 - acc: 0.5752 - val_loss: 1.2873 - val_acc: 0.6455\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 3s 255us/sample - loss: 1.3286 - acc: 0.6355\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5376 - acc: 0.5716 - val_loss: 1.3288 - val_acc: 0.6355\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 3s 253us/sample - loss: 1.2696 - acc: 0.6574\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5211 - acc: 0.5786 - val_loss: 1.2698 - val_acc: 0.6574\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 2s 248us/sample - loss: 1.2946 - acc: 0.6488\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5244 - acc: 0.5808 - val_loss: 1.2947 - val_acc: 0.6488\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 2s 244us/sample - loss: 1.4693 - acc: 0.5877\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5267 - acc: 0.5788 - val_loss: 1.4697 - val_acc: 0.5877\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 2s 244us/sample - loss: 1.3137 - acc: 0.6423\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5274 - acc: 0.5798 - val_loss: 1.3138 - val_acc: 0.6423\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 3s 252us/sample - loss: 1.3232 - acc: 0.6439\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5183 - acc: 0.5817 - val_loss: 1.3237 - val_acc: 0.6439\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 2s 248us/sample - loss: 1.2891 - acc: 0.6603\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 1.5110 - acc: 0.5848 - val_loss: 1.2895 - val_acc: 0.6603\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 2s 246us/sample - loss: 1.2798 - acc: 0.6566\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.5096 - acc: 0.5862 - val_loss: 1.2800 - val_acc: 0.6566\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 3s 252us/sample - loss: 1.2725 - acc: 0.6582\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5139 - acc: 0.5860 - val_loss: 1.2726 - val_acc: 0.6582\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 3s 254us/sample - loss: 1.2281 - acc: 0.6831\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 1.5182 - acc: 0.5857 - val_loss: 1.2284 - val_acc: 0.6831\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 2s 248us/sample - loss: 1.2701 - acc: 0.6617\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.5141 - acc: 0.5858 - val_loss: 1.2703 - val_acc: 0.6617\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 2s 246us/sample - loss: 1.6276 - acc: 0.5387\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5072 - acc: 0.5879 - val_loss: 1.6277 - val_acc: 0.5387\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 2s 247us/sample - loss: 1.3288 - acc: 0.6327\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5077 - acc: 0.5898 - val_loss: 1.3290 - val_acc: 0.6327\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 1.2224 - acc: 0.6782\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.5035 - acc: 0.5891 - val_loss: 1.2226 - val_acc: 0.6782\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 2s 226us/sample - loss: 1.3095 - acc: 0.6534\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 1.5100 - acc: 0.5901 - val_loss: 1.3097 - val_acc: 0.6534\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 2s 245us/sample - loss: 1.2743 - acc: 0.6591\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5053 - acc: 0.5924 - val_loss: 1.2746 - val_acc: 0.6591\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 2s 246us/sample - loss: 1.2614 - acc: 0.6724\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5058 - acc: 0.5942 - val_loss: 1.2615 - val_acc: 0.6724\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 2s 249us/sample - loss: 1.2515 - acc: 0.6681\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5068 - acc: 0.5913 - val_loss: 1.2517 - val_acc: 0.6681\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 2s 241us/sample - loss: 1.4729 - acc: 0.5922\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.5069 - acc: 0.5929 - val_loss: 1.4734 - val_acc: 0.5922\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 2s 244us/sample - loss: 1.3573 - acc: 0.6272\n",
            "1250/1250 [==============================] - 39s 32ms/step - loss: 1.5091 - acc: 0.5958 - val_loss: 1.3577 - val_acc: 0.6272\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 2s 242us/sample - loss: 1.3074 - acc: 0.6536\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4951 - acc: 0.5968 - val_loss: 1.3078 - val_acc: 0.6536\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 1.2600 - acc: 0.6744\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.5101 - acc: 0.5918 - val_loss: 1.2603 - val_acc: 0.6744\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 2s 244us/sample - loss: 1.2806 - acc: 0.6668\n",
            "1250/1250 [==============================] - 41s 32ms/step - loss: 1.4933 - acc: 0.6033 - val_loss: 1.2808 - val_acc: 0.6668\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 2s 240us/sample - loss: 1.3037 - acc: 0.6710\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.5056 - acc: 0.5963 - val_loss: 1.3042 - val_acc: 0.6710\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 2s 243us/sample - loss: 1.4693 - acc: 0.6034\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.5049 - acc: 0.5983 - val_loss: 1.4696 - val_acc: 0.6034\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 2s 247us/sample - loss: 1.2329 - acc: 0.6759\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 1.4929 - acc: 0.6035 - val_loss: 1.2330 - val_acc: 0.6759\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 2s 225us/sample - loss: 1.2195 - acc: 0.6908\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 1.4960 - acc: 0.5975 - val_loss: 1.2198 - val_acc: 0.6908\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 2s 241us/sample - loss: 1.2932 - acc: 0.6617\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.4958 - acc: 0.5986 - val_loss: 1.2935 - val_acc: 0.6617\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 2s 244us/sample - loss: 1.2698 - acc: 0.6667\n",
            "1250/1250 [==============================] - 41s 32ms/step - loss: 1.4910 - acc: 0.6019 - val_loss: 1.2701 - val_acc: 0.6667\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 2s 246us/sample - loss: 1.4433 - acc: 0.5996\n",
            "1250/1250 [==============================] - 41s 32ms/step - loss: 1.4984 - acc: 0.6018 - val_loss: 1.4438 - val_acc: 0.5996\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 2s 242us/sample - loss: 1.2623 - acc: 0.6749\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4957 - acc: 0.5995 - val_loss: 1.2626 - val_acc: 0.6749\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 2s 242us/sample - loss: 1.3235 - acc: 0.6492\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4878 - acc: 0.6050 - val_loss: 1.3240 - val_acc: 0.6492\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 2s 242us/sample - loss: 1.2752 - acc: 0.6693\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4997 - acc: 0.5996 - val_loss: 1.2755 - val_acc: 0.6693\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 1.2336 - acc: 0.6837\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 1.4951 - acc: 0.6038 - val_loss: 1.2338 - val_acc: 0.6837\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 2s 224us/sample - loss: 1.2861 - acc: 0.6612\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 1.4930 - acc: 0.6008 - val_loss: 1.2866 - val_acc: 0.6612\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 2s 241us/sample - loss: 1.1985 - acc: 0.7005\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4955 - acc: 0.5992 - val_loss: 1.1989 - val_acc: 0.7005\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 2s 245us/sample - loss: 1.3223 - acc: 0.6514\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4893 - acc: 0.6035 - val_loss: 1.3225 - val_acc: 0.6514\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 2s 243us/sample - loss: 1.2722 - acc: 0.6789\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4962 - acc: 0.6035 - val_loss: 1.2725 - val_acc: 0.6789\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 2s 244us/sample - loss: 1.2472 - acc: 0.6766\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 1.4886 - acc: 0.6025 - val_loss: 1.2473 - val_acc: 0.6766\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 2s 241us/sample - loss: 1.2333 - acc: 0.6886\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4886 - acc: 0.6038 - val_loss: 1.2337 - val_acc: 0.6886\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 2s 228us/sample - loss: 1.2980 - acc: 0.6574\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 1.4869 - acc: 0.6027 - val_loss: 1.2986 - val_acc: 0.6574\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 2s 241us/sample - loss: 1.4303 - acc: 0.6090\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4899 - acc: 0.6034 - val_loss: 1.4306 - val_acc: 0.6090\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 2s 244us/sample - loss: 1.1786 - acc: 0.7002\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 1.4867 - acc: 0.6045 - val_loss: 1.1787 - val_acc: 0.7002\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 2s 239us/sample - loss: 1.2870 - acc: 0.6699\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4723 - acc: 0.6106 - val_loss: 1.2872 - val_acc: 0.6699\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 2s 246us/sample - loss: 1.4135 - acc: 0.6090\n",
            "1250/1250 [==============================] - 39s 32ms/step - loss: 1.4855 - acc: 0.6068 - val_loss: 1.4139 - val_acc: 0.6090\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 2s 243us/sample - loss: 1.3404 - acc: 0.6337\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4913 - acc: 0.6042 - val_loss: 1.3407 - val_acc: 0.6337\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 2s 227us/sample - loss: 1.2827 - acc: 0.6744\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 1.4821 - acc: 0.6055 - val_loss: 1.2830 - val_acc: 0.6744\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 2s 218us/sample - loss: 1.1962 - acc: 0.7004\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.4823 - acc: 0.6046 - val_loss: 1.1965 - val_acc: 0.7004\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 2s 241us/sample - loss: 1.2219 - acc: 0.6918\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4773 - acc: 0.6077 - val_loss: 1.2221 - val_acc: 0.6918\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 2s 248us/sample - loss: 1.2249 - acc: 0.6924\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4868 - acc: 0.6048 - val_loss: 1.2250 - val_acc: 0.6924\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 2s 242us/sample - loss: 1.2044 - acc: 0.7004\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4835 - acc: 0.6063 - val_loss: 1.2047 - val_acc: 0.7004\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 2s 226us/sample - loss: 1.2445 - acc: 0.6786\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 1.4806 - acc: 0.6098 - val_loss: 1.2449 - val_acc: 0.6786\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 2s 242us/sample - loss: 1.2547 - acc: 0.6741\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4751 - acc: 0.6080 - val_loss: 1.2549 - val_acc: 0.6741\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 2s 242us/sample - loss: 1.2461 - acc: 0.6793\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 1.4855 - acc: 0.6065 - val_loss: 1.2465 - val_acc: 0.6793\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 2s 240us/sample - loss: 1.2691 - acc: 0.6772\n",
            "1250/1250 [==============================] - 39s 32ms/step - loss: 1.4884 - acc: 0.6054 - val_loss: 1.2694 - val_acc: 0.6772\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 1.2577 - acc: 0.6724\n",
            "1250/1250 [==============================] - 38s 31ms/step - loss: 1.4859 - acc: 0.6033 - val_loss: 1.2579 - val_acc: 0.6724\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 2s 220us/sample - loss: 1.2737 - acc: 0.6663\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 1.4744 - acc: 0.6109 - val_loss: 1.2740 - val_acc: 0.6663\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 3s 251us/sample - loss: 1.2728 - acc: 0.6704\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4838 - acc: 0.6095 - val_loss: 1.2730 - val_acc: 0.6704\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 2s 242us/sample - loss: 1.2483 - acc: 0.6774\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4887 - acc: 0.6053 - val_loss: 1.2486 - val_acc: 0.6774\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 2s 234us/sample - loss: 1.2995 - acc: 0.6584\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4824 - acc: 0.6062 - val_loss: 1.3000 - val_acc: 0.6584\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 1.2298 - acc: 0.6843\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 1.4791 - acc: 0.6069 - val_loss: 1.2301 - val_acc: 0.6843\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 2s 250us/sample - loss: 1.1853 - acc: 0.7022\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 1.4673 - acc: 0.6114 - val_loss: 1.1855 - val_acc: 0.7022\n",
            "Epoch 88/100\n",
            " 601/1250 [=============>................] - ETA: 20s - loss: 1.4720 - acc: 0.6113Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "og5QCBOT3iAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b30dac64-57e1-4b34-e122-ed55593b6763"
      },
      "cell_type": "code",
      "source": [
        "model.save('model3_wDataAugmentation.h5')\n",
        "print(\"Model saved to disk\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hKoLuAlSDraX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model1= keras.models.load_model('model1_baseline.h5')\n",
        "# # model1.summary()\n",
        "# model2= keras.models.load_model('model2_wDropout&Regularizer.h5')\n",
        "# # model2.summary()\n",
        "model3= keras.models.load_model('model3_wDataAugmentation.h5')\n",
        "# model3.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P5bPqonHD0NF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9c34b12d-5bf1-42cb-932b-a1b99014d2e4"
      },
      "cell_type": "code",
      "source": [
        "# loss1, acc1 = model1.evaluate(test_images, test_labels)\n",
        "# print(\"Restored Baseline model, accuracy: {:5.2f}%\".format(100*acc1))\n",
        "\n",
        "# loss2, acc2 = model2.evaluate(test_images, test_labels)\n",
        "# print(\"Restored model with Dropout and Regularizations, accuracy: {:5.2f}%\".format(100*acc2))\n",
        "\n",
        "loss3, acc3 = model3.evaluate(test_images, test_labels)\n",
        "print(\"Restored model with Dropout, Regularizations and Data Augmentation, accuracy: {:5.2f}%\".format(100*acc3))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 217us/sample - loss: 1.3618 - acc: 0.6412\n",
            "Restored model with Dropout, Regularizations and Data Augmentation, accuracy: 64.12%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9VtvyjKGEDT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# X_train = train_images_1.reshape(train_images_1.shape[0], 32, 32, 3)\n",
        "# y_train_Kfold = np_utils.to_categorical(train_labels_1, nb_classes)\n",
        "# X_train_Kfold = (X_train/255).astype('float32')\n",
        "\n",
        "\n",
        "# cvscores = []\n",
        "# cv = KFold(n_splits=5, random_state=42, shuffle=False) \n",
        "# for train_index, test_index in cv.split(X_train_Kfold):\n",
        "  \n",
        "#     ## K-fold Split of the dataset(where K=5, 4 of the folds are for training and 1 of them is for validating)\n",
        "#     X_train, X_val, y_train, y_val = X_train_Kfold[train_index], X_train_Kfold[test_index], y_train_Kfold[train_index], y_train_Kfold[test_index]\n",
        "\n",
        "#     ## Compile the optimizer and train \n",
        "#     loaded_model_2.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "#     loaded_model_2.fit(X_train, y_train, epochs=5, batch_size=40)\n",
        "\n",
        "#     ## Get the score on the test dataset\n",
        "#     scores = loaded_model_2.evaluate(X_test, y_test)\n",
        "#     cvscores.append(scores[1] * 100)\n",
        "    \n",
        "# print(\"K-fold cross validation is DONE...\")\n",
        "# print(\"Cross-validation Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}